### 第三章

#### 3.5 下面设计的优点和缺点分别是什么? 分别从操作系统层面和用户层面来阐述。

• 同步和异步通信
• 自动和显式缓冲
• 复制传送和引用传送
• 固定大小和可变大小消息
答：
同步和异步通信： 同步，对于发送进程阻塞，直到消息被接收进程或邮箱所接收；对于接收进程阻塞，直到有消息可用。而异步通信不存在阻塞问题。使用同步通信设计者则无需关心生产者—消费者问题。相比较使用异步通信，可以提高通信系统的效率。
自动或显式缓冲： 自动缓冲提供了一个无限长度的队列，从而保证了发送者在发送消息时不会遇到阻塞。如何提供自动缓存机制因系统而异。当保证有足够大内存的时候，也会有许多内存被浪费。而在缓存大小明确的情况下，发送者仅当临时队列溢出的时候才会被阻塞。此策略下内存很少会被浪费。
**复制发送和引用发送： 复制发送不允许接收者改变参数的状态，引用发送是允许的。引用发送允许的优点之一是它允许程序员写一个分布式版本的一个集中的应用程序。如，Java’s RMI 公司提供两种发送，但引用传递一个参数需要声明这个参数是一个远程对象。**
固定大小和可变大小消息： 如果只能发送定长消息，那么系统级的实现十分简单，但是却使编程任务变得更加困难；相反的是，可变大小消息要求更复杂的系统级实现，但是编程任务变得较为简单。

### 第四章

#### 4.1 举两个多线程程序设计的例子，其中多线程的性能比单线程差

- 顺序程序

```c++
//单线程

add(a,b,c,d){

return a+b+c+d;

}

//多线程

add(a,b,c,d){

return thread1.add(a,b)+thread2.add(c,d)

}
```

#### 4.4 同一进程间的线程究竟共享哪些资源呢，而又各自独享哪些资源呢？

共享的资源有：

a. 堆 由于堆是在进程空间中开辟出来的，所以它是理所当然地被共享的；因此 new 出来的都是共享的（16 位平台上分全局堆和局部堆，局部堆是独享的）

b. 全局变量 它是与具体某一函数无关的，所以也与特定线程无关；因此也是共享的

c. 静态变量 虽然对于局部变量来说，它在代码中是“放”在某一函数中的，但是其存放位置和全局变量一样，存于堆中开辟的.bss 和.data 段，是共享的

d. 文件等公用资源 这个是共享的，使用这些公共资源的线程必须同步。Win32 提供了几种同步资源的方式，包括信号、临界区、事件和互斥体。

独享的资源有：

a. 栈 栈是独享的

b. 寄存器 这个可能会误解，因为电脑的寄存器是物理的，每个线程去取值难道不一样吗？其实线程里存放的是副本，包括程序计数器 PC

### 第五章

#### 5.1 为什么对调度程序而言，区分 CPU 约束程序和 I/O 约束程序很重要？

答：

CPU 的成功调度依赖于进程的如下属性：进程执行由 CPU 执行和 I/O 等待周期组成，进程在这两个状态之间切换。进程执行从 CPU 区间开始，在这之后是 I/O 区间，接着是另一个 CPU 区间，如此进行下去；最终，最后的 CPU 区间通过系统请求终止执行。这些 CPU 区间的长度呈现出指数或超指数形式的频率曲线，具有大量短 CPU 区间和少量长 CPU 区间。I/O 约束程序通常具有很多短 CPU 区间。CPU 约束程序可能有少量的长 CPU 区间。所以区分出 CPU 约束程序和 I/O 约束程序能够使调度程序更好的执行和选择合适的 CPU 调度算法。

CPU 约束型进程可以利用整个时间片，且不会做任何阻碍 I/O 操作的工作；另一方面，I/O 约束型进程有在运行 I/O 操作前只运行很少数量的计算机操作的性质。这种进程一般不会使用很多的 CPU。所以，通过给 I/O 约束型进程优先权和允许在 CPU 约束型进程之前运行，可以很好地利用计算机资源。

#### Discuss how the following pairs of scheduling criteria conflict in certain settings.

a. CPU utilization and response time
b. Average turnaround time and maximum waiting time
c. I/O device utilization and CPU utilization

Answer:

- **CPU utilization and response time:** CPU utilization is increased if the overheads associated with context switching is minimized. The context switching overheads could be lowered by performing context switches infrequently. This could however result in increasing the response time for processes.

  - 解释：频繁地进行上下文切换会导致 CPU 运行时间减少，解决方案有两个：减少上下文切换的开销；减少上下文切换的频率。如果是后者，相当于相同的时间内，做的事情少了，切换的时间不变，那么 CPU 运行（实则划水）的时间就会增加，CPU 利用率就高，但这会导致有些进程等待的时间变长，从而导致响应时间变长。

- **Average turnaround time and maximum waiting time:** Average turnaround time is minimized by executing the shortest tasks first. Such a scheduling policy could however starve long-running tasks and thereby increase their waiting time.
  - 通过优先执行最短的任务，可将平均周转时间降至最低。不过，这种调度策略可能会使运行时间较长的任务处于饥饿状态，从而增加其等待时间。
- **I/O device utilization and CPU utilization:** CPU utilization is maximized by running long-running CPU-bound tasks without performing context switches. I/O device utilization is maximized by scheduling I/O-bound jobs as soon as they become ready to run, thereby incurring the overheads of context switches.
  - 通过运行长期运行的 CPU 绑定任务而不执行上下文切换，可最大限度地提高 CPU 利用率。通过在 I/O 绑定任务准备就绪时立即对其进行调度，从而产生上下文切换的开销，最大限度地提高 I/O 设备利用率。

#### 5.10 Explain the differences in the degree to which the following scheduling algorithms discriminate in favor of short processes:

a. FCFS — discriminates against short jobs since any short jobs arriving after long jobs will have a longer waiting time.
b. RR — treats all jobs equally (giving them equal bursts of CPU time) so short jobs will be able to leave the system faster since they will finish first.
c. Multilevel feedback queues — work similar to the RR algorithm— they discriminate favorably toward short jobs.
